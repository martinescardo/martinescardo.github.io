
\overhead{Appendix}

\overhead{Two-move, history-dependent games}

We have sets of moves $X_0$ and $X_1$. 

Once a move $x_0
\in X_0$ has been played, the allowed moves form a set $S_{x_0}
\subseteq X_1$ that depends on $x_0$. 

\vfill

We want to account for situations such as
\[
\forall x_0 \in X_0 \exists x_1 \in S_{x_0} \,\,p(x_0,x_1).
\]

\overhead{Example}

\[
\forall x_0 \in X_0 \exists x_1 \in S_{x_0} \,\,p(x_0,x_1).
\]
In every pub there are a man $a_0$ and a woman $a_1$
older than $a_0$ such that if $a_0$ buys a drink to $a_1$, then every
man buys a drink to an older woman. 


$S_{x_0}$ is the set of women
older than~$x_0$. 

Formalized by considering two
quantifiers, the second of which has a parameter:
\[
\phi_0 \in K R X_0, \qquad \phi_1 \colon X_0 \to K R X_1.
\]
In our example $\forall x_0 \in X_0 \exists x_1 \in S_{x_0} \,\,p(x_0,x_1)$,
\begin{eqnarray*}
\phi_0(q) & = & \forall x_0 \in X_0 \, q(x_0), \\
\phi_1(x_0)(q) & = & \exists x_1 \in S_{x_0} \,\,q(x_1).
\end{eqnarray*}
Their history-dependent product is defined as the history-free product
considered before, taking care of instantiating the parameter
appropriately:
\[
(\phi_0 \otimes \phi_1)(p)=\phi_0(\lambda x_0.\phi_1(x_0)(\lambda x_1.p(x_0,x_1))).
\]
Similarly, given a selection function and a family of selection functions,
\[
\varepsilon_0 \in J R X_0, \qquad \varepsilon_1 \colon X_0 \to J R X_1,
\]
we define their history-dependent product
\begin{eqnarray*}
  (\varepsilon_0 \otimes \varepsilon_1)(p) & = & (a_0,a_1) \\
        & \text{where} & a_0 = \varepsilon_0(\lambda x_0.\overline{\varepsilon_1}(x_0)(\lambda x_1.p(x_0.x_1))) \\
        & & a_1 = \varepsilon_1(a_0)(\lambda x_1.p(a_0,x_1)),
\end{eqnarray*}
where it is understood that $\overline{\varepsilon_1}(x_0) =
\overline{\varepsilon_1(x_0)}$.  And then again we have
\[
\overline{\varepsilon_0} \otimes \overline{\varepsilon_1}
= \overline{\varepsilon_0 \otimes \varepsilon_1}.
\]
This amounts to saying that if $\varepsilon_0$ is a selection
function for the quantifier $\phi_0$, and if $\varepsilon_1(x_0)$ is a
selection function for the quantifier $\phi_1(x_0)$ for every $x_0 \in
X_0$, then $\varepsilon_0 \otimes \varepsilon_1$ is a selection
function for the quantifier $\phi_0 \otimes \phi_1$.

We can iterate this if we are given a sequence of history dependent
selection functions 
\[
\varepsilon_n \colon \prod_{i < n} X_i \to J R X_n.
\]
We do this by induction, using the binary history dependent product in
the induction step:

\begin{footnotesize}
\[
  \bigotimes_{i < 0} \varepsilon_i   =   \lambda p.(),
\]
\[  
\bigotimes_{i < n+1} \varepsilon_i   =  \varepsilon_0() \otimes \lambda x_0.\bigotimes_{i < n} (\lambda (x_1,\dots,x_{i}).\varepsilon_{i+1}(x_0,\dots,x_{i})).
\]
\end{footnotesize}

\noindent



\overhead{Uniform continuity and the fan functional}

Functions \verb+f :: [Bool] -> z+ are continuous. 

If \verb+z+ is discrete (has equality), this means that for every
input~\verb+a+, there is an index $n$ depending on \verb+a+, such that
the answer doesn't depend on positions of \verb+a+ with indices $i \ge
n$. 

Now,
the Cantor space is compact, and there are theorems in topology that
say that continuous functions defined on compact
spaces are \emph{uniformly continuous}. 

In our case, this means that
there is a \emph{single} index~$n$, independent of the input,
such that the function \verb+f :: [Bool] -> z+ doesn't look at indices~$i
\ge n$ in order to produce the output. 

Another way of saying this is
that if two inputs \verb+a+ and \verb+b+ of the function~\verb+f+
agree in the first~$n$ positions, then they produce the same
output. This criterion can be coded in Haskell, and hence such an~$n$
can be computed. The functional that computes it is called the
\emph{fan functional}, and is known since the 1950's or even
earlier.

\overhead{Haskell code}

\begin{verbatim}
 fan :: Eq z => ([Bool] -> z) -> Int
 fan f = least(\n -> forevery(\a -> forevery(\b -> 
            agree n a b --> (f a == f b))))

 least :: (Int -> Bool) -> Int
 least p = if p 0 then 0 else 1+least(\n -> p(n+1))

 forsome, forevery :: K Bool [Bool]
 forsome = overline findCantor 
 forevery p = not(forsome(not.p))
 
 agree :: Int -> [Bool] -> [Bool] -> Bool
 agree n a b = take n a == take n b

 (-->) :: Bool -> Bool -> Bool
 p --> q = not p || q
\end{verbatim}

\overhead{Trivial example.}
\begin{verbatim}
 *Main> fan(\a -> a !! 5  &&  a !! 6)
 7
\end{verbatim}


\overhead{Explanation}

We are not interested in computing the fan functional.  

What
is relevant is that once we know the modulus of uniform
continuity~$n$, it is enough to inspect $2^n$ cases to figure out the
complete behaviour of the function. 

\overhead{Magic?}

If there is any magic in the above
algorithm for quantifying over the Cantor space and hence deciding
equality of functions defined on the Cantor space, it amounts to the
facts that 
\begin{enumerate}
\item 
this $n$ is not explicitly calculated by the
quantification procedure (although it can be calculated with the fan
functional that \emph{uses} this procedure), 
\item
very often, a
small portion of the $2^n$ cases actually need to be checked in the
quantification procedure (this has to do with lazy evaluation).  
\end{enumerate}
If we
move from \verb+[Bool]+ to \verb+[Integer]+, then compactness and
uniform continuity fail, and moreover equality of functions
\verb+(Integer -> Integer) -> z+ is no longer decidable.

\overhead{}

We observe that \verb+sequence+ and \verb+hsequence+, although defined
for all monads, usually don't produce convergent computations in
monads other than \verb+J r+ when supplied with infinite lists.
Moreover, the type \verb+r+ has to have decidable equality for
infinite products of selection functions to be total (topologically,
it has to be discrete)~\cite{EO(2009)}. The termination proof
is non-trivial and relies on the so-called \emph{bar induction}
principle. In particular, infinite products of quantifiers
\emph{cannot} be computed with \verb+sequence+ or~$\bigotimes$, as
also shown in~\cite{EO(2009)}, with a continuity argument.

One can easily understand this kind of phenomenon with the list monad.
In the finite case, \verb+sequence+ computes cartesian products in the
lexicographic order:
\begin{verbatim}
 Prelude> sequence [[0,1],[0,1],[0,1]]      
 [[0,0,0],[0,0,1],[0,1,0],[0,1,1],
  [1,0,0],[1,0,1],[1,1,0],[1,1,1]]
\end{verbatim}
But now the elements of the countable cartesian product
$\{0,1\}^\omega$ cannot be arranged in an infinite list, by Cantor's
diagonal argument, and when we attempt to list them using
\verb+sequence+, we get a divergent computation, which in practice
aborts for lack of memory:
\begin{verbatim}
 Prelude> sequence (repeat [0,1])        
 *** Exception: stack overflow
\end{verbatim}
It is reassuring to see Haskell refusing to give an answer to an
impossible question.

Apart from the selection monad, a monad for which we know that
\verb+sequence+ and \verb+hsequence+ converge for infinite lists is
the identity monad:
\begin{verbatim}
 newtype Id x = Id { di :: x } deriving (Show)
  
 instance Monad Id where
     return a     = Id a   
     (Id x) >>= f = f x
\end{verbatim}
Here \verb+di+ removes the tag \verb+Id+.  Bearing in mind that
semantically \verb+Id+ is the identity, \verb+sequence+ essentially
does nothing:
\begin{verbatim}
 *Main> sequence [Id 1,Id 2,Id 3]
 Id [1,2,3]
\end{verbatim}
However, \verb+hsequence+ is much more interesting, as it amounts to
course-of-values recursion. The type of \verb+hsequence+ in this case
reduces to \verb+[[x] -> Id x] -> Id[x]+, and, according to the
development of the previous sections, the $n$th function in the input
list $[f_0,f_1,f_2,\dots]$ is intended to have~$n$ arguments. What
\verb+hsequence+ computes, then, is the sequence $\mathtt{Id}[x_0,x_1,x_2,\dots]$
defined by course-of-values induction as
\[
\mathtt{Id} x_{n} = f_n[x_0,\dots,x_{n-1}].
\]
As an example, we enrich the literature
with yet another way of computing the Fibonacci sequence:
\begin{verbatim}
fibonacci :: [Integer]
fibonacci = di(hsequence (repeat f))
  where f [ ] = Id 1
        f [_] = Id 1
        f xs  = Id((xs !! (i - 1)) + (xs !! i))
          where i = length xs - 1
\end{verbatim}
Notice that the definition of $f$ is \emph{not} recursive. As
discussed above, the recursion is performed by~\verb+hsequence+.  Here
is what we get when we run it:
\begin{verbatim}
 *Main> take 10 fibonacci
 [1,1,2,3,5,8,13,21,34,55]
\end{verbatim}

Now, the identity functional and primitive recursion functional are
realizers of the intuitionistic axioms of choice and of dependent
choice respectively.  It turns out that when the monad is \verb+J r+
rather than \verb+Id+, the functionals \verb+sequence+ and
\verb+hsequence+ are instead realizers of the \emph{classical} axioms
of choice and of (slightly generalized) dependent choice, which brings
us to the subject of the next section.

% Going back to section 6.3, maybe it's worth pointing out that in order
% to defined course-of-values recursion using the identity monad we in
% fact only need hsequence for finite sequences (i.e. the finite
% dependent product). The point is that in order to compute f n we only
% need to iterate the product n times.

\overhead{The Double-Negation Shift} 

This last section of the tutorial is a rather brief excursion to
Proof-Theory Land with many gaps. We look at the $J$ and $K$ monads
from the propositions-as-types and proofs-as-programs point of view,
for which the dependently-typed functional language
Agda~\cite{bove:dybjer} is more appropriate than Haskell. We revert to
the notation defined before the previous section, in order to avoid
the distracting tags that arise in the definitions of the monads given
in Section~\ref{tags}.

\overhead{The G\"odel--Gentzen negative translation and the continuation monad}

It is well known that $K_R A = ((A \to R) \to R)$ can be seen as a
generalized double-negation operator, reducing to standard double
negation when $R=\bot$ (the proposition \emph{absurdity}, or the empty
type). In order to avoid notational clutter, we write $K A = K_R A$,
relying on the reader's ability to infer the subscript~$R$ from the
context. The G\"odel--Gentzen's \emph{negative-translation} prefixes
double negations in front of atomic propositions, disjunctions and
existential quantifiers, leaving implications, conjunctions and
universal quantifiers unchanged.  A more general translation prefixes
$K$ instead. Given a formula~$A$, we denote its translation by
$A^{K}$.  The reason for considering this translation is that given
any \emph{classical} proof of $A$ one can algorithmically find an
\emph{intuitionistic} proof of its translation~$A^{K}$.

% Page 11, second column. "we denote the translation by A^K" --> "we
% denote its translation by A^K" Page 11, second column. In fact the
% proof of A^K is in minimal logic, but I'm not sure we should expand on
% this?

From the point of view of proofs-as-programs, this algorithm amounts
to one of the possible forms of the well-known \emph{continuation
  passing style} translation. If one works through the technical
details, one sees that what makes everything work is that fact that
$K$ is a monad. In particular, algebras of the monad, which are
propositions $A$ satisfying
\[
K A \to A,
\]
arise: every translated formula can be shown to be an algebra. In the
particular case $R=\bot$, this amounts to saying that $A$ satisfies
the double-negation elimination rule $\neg \neg A \to A$, which is a
classical principle. Thus, the translation forces this principle to
hold intuitionistically.

% % Aborted:
% [[Add remark. EM falls out much more easily if we use $J$. Will
% explain how (because Ulrich Berger emphatically liked it when he
% visited).]]

\overhead{The Peirce translation, the selection monad and call/cc}

One can routinely repeat the above development with any monad. When
this is done for $J A = J_R A = ((A \to R) \to A)$, algebras 
\[
J A \to A
\]
are propositions that satisfy Peirce's Law \[ ((A \to R) \to A) \to
A.\] Usually the continuation monad is invoked to explain call/cc, but
a more natural explanation is obtained in terms of the selection monad. 

% Because
% there is a monad morphism $J \to K$, any $K$-algebra is a
% $J$-algebra by composition with the morphism, and, in particular,
% because the multiplication $K K A \to K A$ of the continuation
% monad shows that $K A$ is a $K$-algebra, it is also a
% $J$-algebra. The resulting functional $J K A \to K A$ amounts to
% call/cc.

% Sec 7.2. I think it is really "J A -> A" which amounts to call/cc,
% not "J K A -> K A", isn't it?

\overhead{Extracting programs from proofs}

\newcommand{\HA}{\operatorname{HA}}
\newcommand{\PA}{\operatorname{PA}}

Let $A = \forall x \exists y \,\, p(x,y)$ be a formula in Heyting
arithmetic with finite types ($\HA^\omega$), where $p$ is decidable.
Given an intuitionistic proof of $A$ one can find a program $t$ (in
G\"odel system $T$, which can be considered as a downgraded version of
Haskell) such that $p(x,t x)$ holds for every~$x$. One can think of
this as follows: (1)~$p$ is the specification of the input-output
relation of a program to be written down, (2)~the given proof of~$A$,
because it is intuitionistic, implicitly carries such a program,
(3)~there is a procedure that exhibits the program given the proof,
(4)~hence rather than writing a program, one can show in
intuitionistic logic that for every input~$x$ there is an output~$y$
satisfying the specification, and get a program automatically, which
by construction satisfies the specification.

Of course, this would be impractical for the average programmer. But
there are many mathematicians and logicians in computer science
departments who are doing just that, using various kinds of mechanical
proof assistants. An added bonus is that there is a large body of
literature with intuitionistic proofs available, and hence potentially
a large body of programs that don't need to be explicitly written
down. The downside is that \emph{formalizing} rigorous proofs is known
to be no easy task. But nevertheless, if this activity is not
practical at the moment, it is certainly very exciting, scientifically
deep, enlightening, and mathematically pleasing.

But what if the proof of $A$ is classical, rather than intuitionistic,
that is in Peano arithmetic with finite types ($\PA^\omega$)? Never
mind. Using the negative translation, one can still extract a program.
However, when one climbs up the logical systems, one gets into
difficulties.  For example, there is no problem in using the axioms of
choice or dependent choice in $\HA^\omega$, because they are
realizable (one can write programs in system $T$ that implement
them). But in $\PA^\omega$, the situation is subtler. What one needs
is to realize the negative translations of the axioms, which is
problematic because they involve existential quantification, which is
altered by the translation.

\overhead{The axiom  of choice and the double-negation shift}

Let $T$ be any of the monads $J$ or $K$. The $T$-translation of the
axiom of choice is
\[
\forall x \,\, T \exists y \,\, A(x,y) \implies \,\, T\exists f \forall
x \,\,A(x,f(x)).
\]
The axiom of choice is the case in which $T$ is the identity monad.
To extract programs from classical proofs in $\PA^\omega$ using the
axiom of choice, one needs to realize the $K$-translation of the axiom
of choice, which is often referred to as the \emph{classical axiom of
  choice}. Spector's idea~\cite{Spector(62)} was to instead realize
the $T$-shift (with $T=K$ and $R=\bot$),
\[
\forall x\,\, T B(x) \implies T\forall x \,\, B(x).
\]
and prove that the intuitionistic axiom of choice together with this
gives the classical axiom of choice.  This is enough to be able to
extract programs from proofs in $\PA^\omega$ extended with
choice. Spector did this for the case where $x$ ranges over natural
numbers, realizing the \emph{axiom of countable choice}. Although
Spector worked with the dialectica interpretation and an extension of
system T with so-called bar recursion, his ideas remain relevant and
crisp. See also~\cite{Berardi(98),BO(02A)}.


It turns out that the history-free product of selection functions
directly realizes the $T$-shift for $T=J$, and that the $J$-shift
implies the $K$-shift when $B$ is in the image of the
$K$-translation~\cite{EO(2010B)}.  In fact, the iterated product of
selection functions provides a alternative and intuitive formulation
of bar recursion.  Formal proofs/programs written in Agda are
available at~\cite{EO(2010):mfps:programs}. It is worth mentioning
that the $J$-shift is not system-$T$ definable, and that to define it
in Agda one has to disable the termination checker (and then users
have to trust us).


\overhead{The axiom of dependent choice}

The previous subsection states that the history-free product of
selection functions realizes the $J$-shift, which in turn gives the
double-negation shift, and the $J$- and $K$-translations of the axiom
of choice. It turns out that the history-dependent product of
selection functions realizes a version of the axiom of dependent
choice. We don't have an implementation of this in Agda yet, but we
are working on this and related things.

\overhead{Where are the (proofs and) programs for this section?}

Proofs/programs, written in Agda~\cite{bove:dybjer}, can be found
at~\cite{EO(2010):mfps:programs}. No previous knowledge of Agda is
necessary: the types of the programs read like usual logical
expressions in ordinary mathematical notation, and their proofs look
like usual functional programs.  There is also no need to run them,
because their types tell us what their behaviour will be. However, we
plan to apply them to give an alternative implementation of the
program extractions from classical proofs using dependent choice
developed by Monika Seisenberger in the system
Minlog~\cite{Seisenberger(2008)}.


\end{document}
